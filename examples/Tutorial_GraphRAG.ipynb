{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåç GraphRAG Core: Climate Intelligence Tutorial\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nunezmatias/grafoRag/blob/main/examples/Tutorial_GraphRAG.ipynb)\n",
    "\n",
    "Welcome to the **GraphRAG Core** tutorial. This notebook demonstrates a next-generation retrieval system designed for scientific discovery.\n",
    "\n",
    "By combining vector search with a causal knowledge graph, we can answer complex questions about climate adaptation, identifying not just *what* is happening, but *why* it matters and what ripple effects it might trigger.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation\n",
    "Install the library directly from GitHub. This will setup the engine and download required dependencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/nunezmatias/grafoRag.git\n",
    "!pip install -q -U google-genai\n",
    "\n",
    "import os\n",
    "from graphrag_core import GraphRAGEngine\n",
    "print('‚úÖ Libraries Installed & Loaded')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize the Engine\n",
    "Initializing the engine is simple. If the climate data is not found locally, it will be automatically downloaded from the cloud storage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = GraphRAGEngine()\n",
    "# Output should say: \"Attempting to download from Google Drive...\" followed by \"System Ready\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run a Deep Research Query\n",
    "We will now perform a complex search. The engine allows you to tune the depth of the investigation:\n",
    "\n",
    "- **`top_k`** estabelece la amplitud tem√°tica.\n",
    "- **`context_k`** controla la profundidad de evidencia por tema.\n",
    "- **`hops`** define el razonamiento causal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your research question\n",
    "query = \"cascading risks of extreme heat and urban floods\"\n",
    "\n",
    "# Execute the Search\n",
    "results = engine.search(\n",
    "    query=query, \n",
    "    top_k=3, \n",
    "    context_k=4, \n",
    "    hops=2\n",
    ")\n",
    "\n",
    "print(f\"--- Research Stats ---\")\n",
    "print(f\"Primary Sources: {results[\"stats\"][\"primary\"]}\")\n",
    "print(f\"Context Expansion: {results[\"stats\"][\"context\"]}\")\n",
    "print(f\"Causal Links:      {results[\"stats\"][\"graph\"]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inspect the Intelligence\n",
    "Verify the quality of the retrieved data before generating the final answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Check the Top Paper\n",
    "if results['papers']:\n",
    "    p = results['papers'][0]\n",
    "    print(f\"üìÑ Top Paper: {p[\"title\"]}\")\n",
    "    print(f\"   Snippet: {p[\"content\"][:200]}...\")\n",
    "\n",
    "# 2. Check Discovered Causal Chains\n",
    "if results['graph_links']:\n",
    "    print(\"\")\n",
    "    print(\"üîó Sample Causal Chains:\")\n",
    "    for link in results['graph_links'][:5]:\n",
    "        print(f\"   {link[\"node1\"]} --[{link[\"relation\"]}]--> {link[\"node2\"]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Construct the Expert Prompt\n",
    "We package the data into a rigorous prompt for the LLM using the engine's built-in template.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = engine.format_prompt(results, query)\n",
    "\n",
    "print(\"Here is your optimized prompt (COPY THIS):\")\n",
    "print(\"--------------------------------------------------\")\n",
    "print(prompt)\n",
    "print(\"--------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Answer with Gemini Flash ‚ö°\n",
    "Send the prompt to Google's Gemini Flash model for final synthesis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.colab import userdata\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "try:\n",
    "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
    "    client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "    print(\"‚úÖ Gemini Client Configured\")\n",
    "except Exception as e:\n",
    "    print(\"‚ö†Ô∏è Error: API Key not found. Please add 'GOOGLE_API_KEY' to Colab Secrets.\")\n",
    "\n",
    "print(\"‚è≥ Generating expert response with Gemini Flash...\")\n",
    "try:\n",
    "    response = client.models.generate_content(\n",
    "        model='gemini-flash-latest',\n",
    "        contents=prompt\n",
    "    )\n",
    "    display(Markdown(\"### ü§ñ Response:\"))\n",
    "    display(Markdown(response.text))\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Generation Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced: Custom Prompt Template\n",
    "Access raw variables to build your own structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_role = \"You are a Data Journalist.\"\n",
    "my_instruction = \"Summarize in 3 bullet points.\"\n",
    "papers_text = \"\"\n",
    "for p in results['papers']:\n",
    "    papers_text += f\"- {p[\"title\"]}: {p[\"content\"][:200]}...\n\"\n",
    "graph_text = \"\"\n",
    "for link in results['graph_links']:\n",
    "    graph_text += f\"- {link[\"node1\"]} causes {link[\"node2\"]}\n\"\n",
    "custom_prompt = (f\"ROLE: {my_role}\nQUESTION: {query}\n\nDATA:\n{papers_text}\n{graph_text}\n\nDO: {my_instruction}\")\n",
    "print(\"--- Custom Prompt Preview ---\")\n",
    "print(custom_prompt[:300] + \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Bonus: Swap the Brain üß†\n",
    "Download and load a new domain dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://github.com/nunezmatias/grafoRag/raw/main/examples/test_brain.zip\n",
    "!unzip -o -q test_brain.zip\n",
    "solar_engine = GraphRAGEngine(vector_db_path='./test_brain/test_db', graph_json_path='./test_brain/test_skeleton.json')\n",
    "solar_results = solar_engine.search(\"How does solar energy affect the grid?\", top_k=2)\n",
    "print(f\"Found {len(solar_results[\"graph_links\"])} causal links in the new brain.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Advanced: Load from Google Drive ‚òÅÔ∏è\n",
    "Share custom knowledge bases via Drive File IDs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_CUSTOM_GDRIVE_ID = \"1iKcEzECN9LTMi3bIq4ocRfFJgvb1dLus\"\n",
    "try:\n",
    "    drive_engine = GraphRAGEngine(gdrive_id=MY_CUSTOM_GDRIVE_ID)\n",
    "    print(\"‚úÖ Custom Brain Loaded\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}