{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83c\udf0d GraphRAG Core: Climate Intelligence Tutorial\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nunezmatias/grafoRag/blob/main/examples/Tutorial_GraphRAG.ipynb)\n",
    "\n",
    "Welcome to the **GraphRAG Core** interactive research tool. This system bridges the gap between unstructured scientific literature and structured causal knowledge.\n",
    "\n",
    "### \ud83e\udde0 How it works (The \"Node-Centric\" Engine)\n",
    "Unlike traditional RAG which just finds text chunks, this engine operates in three cognitive layers:\n",
    "1.  **Scope (Vector Search):** Identifies the key *Topics* (Nodes) relevant to your query.\n",
    "2.  **Depth (Context Expansion):** For each identified topic, it dives deep to find specific evidence, ensuring high semantic density.\n",
    "3.  **Causality (Graph Traversal):** It follows the causal links in the Knowledge Graph to find downstream effects (e.g., *Heat* -> *Drought* -> *Food Security*).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation\n",
    "We install the `graphrag_core` library (which includes the engine and data) and the Google GenAI SDK for the final answer generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/nunezmatias/grafoRag.git\n",
    "!pip install -q -U google-genai\n",
    "\n",
    "import os\n",
    "from graphrag_core import GraphRAGEngine\n",
    "print(\"\u2705 Libraries Installed & Loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize the Knowledge Engine\n",
    "We initialize the engine without arguments. It will automatically detect if the Climate Database is missing and download it (~300MB) from the cloud to your Colab environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = GraphRAGEngine()\n",
    "# Watch the output for the download progress..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run a Deep Research Query\n",
    "Here you can tune the research parameters. Think of this as adjusting the lens of a microscope.\n",
    "\n",
    "### \ud83c\udf9b\ufe0f Parameters Explained:\n",
    "*   **`top_k` (Breadth):** How many distinct *Topics* (Nodes) to investigate. Higher means a broader scope.\n",
    "*   **`context_k` (Depth):** How many scientific abstracts to read *per topic*. Higher means more nuance and consensus checking.\n",
    "*   **`hops` (Causality):** How many steps to traverse in the causal graph.\n",
    "    *   `1`: Direct effects (Heat -> Health)\n",
    "    *   `2`: Cascading effects (Heat -> Drought -> Agriculture)\n",
    "\n",
    "Try changing the query below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your research question\n",
    "query = \"cascading risks of extreme heat and urban floods\"\n",
    "\n",
    "# Execute the Search\n",
    "results = engine.search(\n",
    "    query=query, \n",
    "    top_k=3,        # Look for 3 main topics\n",
    "    context_k=4,    # Read 4 papers per topic\n",
    "    hops=2          # Find 2nd order consequences\n",
    ")\n",
    "\n",
    "print(f\"--- Research Stats ---\")\n",
    "print(f\"Primary Sources: {results['stats']['primary']}\")\n",
    "print(f\"Context Expansion: {results['stats']['context']}\")\n",
    "print(f\"Causal Links:      {results['stats']['graph']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inspect the Retrieved Intelligence\n",
    "Before generating the answer, let's verify what the engine found. This \"White Box\" approach builds trust in the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Check the Top Paper\n",
    "if results['papers']:\n",
    "    p = results['papers'][0]\n",
    "    print(f\"\ud83d\udcc4 Top Paper: {p['title']}\")\n",
    "    print(f\"   Snippet: {p['content'][:200]}...\")\n",
    "\n",
    "# 2. Check Discovered Causal Chains\n",
    "if results['graph_links']:\n",
    "    print(\"\n\ud83d\udd17 Sample Causal Chains:\")\n",
    "    for link in results['graph_links'][:5]:\n",
    "        print(f\"   {link['node1']} --[{link['relation']}]--> {link['node2']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Construct the Expert Prompt\n",
    "The engine uses a specialized template to package this data for the LLM. It explicitly instructs the model to:\n",
    "1.  **Triangulate** data (Graph vs Text).\n",
    "2.  Identify **Cascades**.\n",
    "3.  Cite sources with **URLs**.\n",
    "\n",
    "You can customize the `system_role` below to change the persona (e.g., \"Policy Maker\", \"Engineer\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = engine.format_prompt(\n",
    "    results,\n",
    "    query,\n",
    "    system_role=\"You are a Senior Climate Adaptation Specialist at the UN.\"\n",
    ")\n",
    "\n",
    "# print(prompt) # Uncomment to see the full massive prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Answer with Gemini Flash \u26a1\n",
    "Finally, we send the prompt to Google's Gemini model to synthesize the final report.\n",
    "\n",
    "**Note:** You need a Google API Key.\n",
    "1. Get it from [Google AI Studio](https://aistudio.google.com/).\n",
    "2. In Colab, click the **Key icon** (Secrets) on the left sidebar.\n",
    "3. Add a new secret named `GOOGLE_API_KEY` with your key value.\n",
    "4. Toggle the \"Notebook access\" switch to ON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.colab import userdata\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Configuraci\u00f3n de la API Key\n",
    "try:\n",
    "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
    "    client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "    print(\"\u2705 Gemini Client Configured\")\n",
    "except Exception as e:\n",
    "    print(\"\u26a0\ufe0f Error: API Key not found. Please add 'GOOGLE_API_KEY' to Colab Secrets.\")\n",
    "\n",
    "# Generar contenido\n",
    "print(\"\u23f3 Generating expert response with Gemini 2.0 Flash...\")\n",
    "try:\n",
    "    response = client.models.generate_content(\n",
    "        model='gemini-2.0-flash',\n",
    "        contents=prompt\n",
    "    )\n",
    "    \n",
    "    # Mostrar la respuesta formateada\n",
    "    display(Markdown(\"### \ud83e\udd16 Response:\"))\n",
    "    display(Markdown(response.text))\n",
    "except Exception as e:\n",
    "    print(f\"\u274c Generation Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}