{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83c\udf0d GraphRAG Core: Climate Intelligence Tutorial\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nunezmatias/grafoRag/blob/main/examples/Tutorial_GraphRAG.ipynb)\n",
    "\n",
    "Welcome to the **GraphRAG Core** tutorial. This library comes **pre-loaded** with a massive Climate Adaptation Knowledge Graph, allowing you to perform deep scientific research instantly.\n",
    "\n",
    "### \ud83d\ude80 Features\n",
    "1. **Plug & Play**: No data setup required. The Climate Knowledge Base is embedded (auto-downloaded).\n",
    "2. **Deep Retrieval**: Finds not just one paper, but the entire context around a topic.\n",
    "3. **Causal Reasoning**: Traverses the graph to find systemic risks (e.g., Heat -> Drought -> Food Insecurity).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation\n",
    "Install the library directly from GitHub. This will setup the engine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/nunezmatias/grafoRag.git\n",
    "!pip install -q -U google-genai\n",
    "\n",
    "import os\n",
    "from graphrag_core import GraphRAGEngine\n",
    "print('\u2705 Libraries Installed & Loaded')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize the Engine\n",
    "We initialize the engine without arguments. It will automatically detect missing data and download the Climate Database (~300MB) from the cloud.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = GraphRAGEngine()\n",
    "# Output should say: \"Attempting to download from Google Drive...\" followed by \"System Ready\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run a Deep Research Query\n",
    "We will now perform a complex search. The engine allows you to tune the depth of the investigation:\n",
    "\n",
    "- **`top_k`** controls breadth (how many distinct topics to start with).\n",
    "- **`context_k`** controls depth (how many papers to read per topic).\n",
    "- **`hops`** controls causal reasoning (how far to travel in the graph).\n",
    "\n",
    "A configuration of `hops=2` allows us to see second-order effects, essential for understanding cascading risks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your research question\n",
    "query = \"cascading risks of extreme heat and urban floods\"\n",
    "\n",
    "# Execute the Search\n",
    "results = engine.search(\n",
    "    query=query, \n",
    "    top_k=3,        # Breadth\n",
    "    context_k=4,    # Depth\n",
    "    hops=2          # Causality\n",
    "))\n",
    "\n",
    "print(f\"--- Research Stats ---\")\n",
    "print(f\"Primary Sources: {results['stats']['primary']}\")\n",
    "print(f\"Context Expansion: {results['stats']['context']}\")\n",
    "print(f\"Causal Links:      {results['stats']['graph']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inspect the Intelligence\n",
    "It is important to verify the quality of the retrieved data. Below, we print the top-ranked paper and a sample of the causal chains discovered by the graph traversal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Check the Top Paper\n",
    "if results['papers']:\n",
    "    p = results['papers'][0]\n",
    "    print(f\"\ud83d\udcc4 Top Paper: {p['title']}\")\n",
    "    print(f\"   Snippet: {p['content'][:200]}...\")\n",
    "\n",
    "# 2. Check Discovered Causal Chains\n",
    "if results['graph_links']:\n",
    "    print(\"\")\n",
    "    print(\"\ud83d\udd17 Sample Causal Chains:\")\n",
    "    for link in results['graph_links'][:5]:\n",
    "        print(f\"   {link['node1']} --[{link['relation']}]--> {link['node2']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Construct the Expert Prompt\n",
    "We use the engine's built-in expert template to package this structured data into a rigorous prompt for the LLM. This template forces the model to triangulate evidence and cite sources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the default expert template designed for this Climate Graph\n",
    "prompt = engine.format_prompt(results, query)\n",
    "\n",
    "print(\"Here is your optimized prompt (COPY THIS):\")\n",
    "print(\"--------------------------------------------------\")\n",
    "print(prompt)\n",
    "print(\"--------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Answer with Gemini Flash \u26a1\n",
    "Finally, we send the generated prompt to Google's Gemini model.\n",
    "\n",
    "**Prerequisite:** Add your API Key to Colab Secrets (Key icon on the left) with the name `GOOGLE_API_KEY`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.colab import userdata\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Configuraci\u00f3n de la API Key\n",
    "try:\n",
    "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
    "    client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "    print(\"\u2705 Gemini Client Configured\")\n",
    "except Exception as e:\n",
    "    print(\"\u26a0\ufe0f Error: API Key not found. Please add 'GOOGLE_API_KEY' to Colab Secrets.\")\n",
    "\n",
    "# Generar contenido\n",
    "print(\"\u23f3 Generating expert response with Gemini 2.0 Flash...\")\n",
    "try:\n",
    "    response = client.models.generate_content(\n",
    "        model='gemini-2.0-flash',\n",
    "        contents=prompt\n",
    "    )\n",
    "    \n",
    "    # Mostrar la respuesta formateada\n",
    "    display(Markdown(\"### \ud83e\udd16 Response:\"))\n",
    "    display(Markdown(response.text))\n",
    "except Exception as e:\n",
    "    print(f\"\u274c Generation Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced: Build Your Own Prompt Template\n",
    "Do you want full control? Here is how you can access the raw variables `papers` and `graph_links` to modify the prompt structure entirely before sending it to the LLM.\n",
    "\n",
    "You can edit the F-String below to change the persona, instructions, or layout.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CUSTOMIZE YOUR TEMPLATE HERE ---\n",
    "my_role = \"You are a Data Journalist writing for a general audience.\"\n",
    "my_instruction = \"Summarize the risks in 3 bullet points. Be concise.\"\n",
    "\n",
    "# 1. Flatten the Papers data into a string\n",
    "papers_text = \"\"\n",
    "for p in results['papers']:\n",
    "    papers_text += f\"- {p['title']}: {p['content'][:200]}...\n\"\n",
    "\n",
    "# 2. Flatten the Graph data into a string\n",
    "graph_text = \"\"\n",
    "for link in results['graph_links']:\n",
    "    graph_text += f\"- {link['node1']} causes {link['node2']}\n\"\n",
    "\n",
    "# 3. Build the F-String (Edit this!)\n",
    "custom_prompt = (f\"ROLE: {my_role}\n\"\n",
    "                 f\"QUESTION: {query}\n\n\"\n",
    "                 f\"SCIENTIFIC EVIDENCE:\n{papers_text}\n\"\n",
    "                 f\"CAUSAL LINKS:\n{graph_text}\n\"\n",
    "                 f\"INSTRUCTION: {my_instruction}\")\n",
    "\n",
    "print(\"--- Custom Prompt Created ---\")\n",
    "print(custom_prompt[:500] + \"... [Truncated]\")\n",
    "\n",
    "# 4. Send Custom Prompt to Gemini\n",
    "print(\"\n\u23f3 Generating Custom Response...\")\n",
    "try:\n",
    "    response = client.models.generate_content(\n",
    "        model='gemini-2.0-flash',\n",
    "        contents=custom_prompt\n",
    "    )\n",
    "    display(Markdown(\"### \ud83e\udd16 Custom Response:\"))\n",
    "    display(Markdown(response.text))\n",
    "except Exception as e:\n",
    "    print(f\"\u274c Generation Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Bonus: Swap the Brain \ud83e\udde0 (Test a New Dataset)\n",
    "Do you want to use this engine for a different domain (e.g., Solar Energy, Finance, Law)?\n",
    "You can swap the underlying \"Brain\" instantly by providing a new Vector DB and Graph JSON.\n",
    "\n",
    "In this example, we will download a small **Test Brain** about Solar Energy from the repository and initialize a new engine with it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Download the Test Brain (Solar Energy) from GitHub\n",
    "!wget https://github.com/nunezmatias/grafoRag/raw/main/examples/test_brain.zip\n",
    "!unzip -o test_brain.zip\n",
    "\n",
    "print(\"\u2705 Test Brain Downloaded & Extracted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Initialize a NEW Engine with this data\n",
    "solar_engine = GraphRAGEngine(\n",
    "    vector_db_path=\"./test_brain/test_db\",\n",
    "    graph_json_path=\"./test_brain/test_skeleton.json\"\n",
    ")\n",
    "\n",
    "# 3. Run a Query on the new domain\n",
    "solar_query = \"How does solar energy affect the grid stability?\"\n",
    "\n",
    "solar_results = solar_engine.search(solar_query, top_k=2)\n",
    "\n",
    "print(f\"\n\ud83d\udd0d Query: {solar_query}\")\n",
    "print(f\"Found {len(solar_results['papers'])} papers and {len(solar_results['graph_links'])} links.\")\n",
    "\n",
    "if solar_results['graph_links']:\n",
    "    link = solar_results['graph_links'][0]\n",
    "    print(f\"\ud83d\udd17 Link Found: {link['node1']} --[{link['relation']}]--> {link['node2']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Advanced: Load your own Brain from Google Drive \u2601\ufe0f\n",
    "If you have your Knowledge Graph and Vector DB packaged in a `.zip` and hosted on Google Drive, you can load it directly by passing its **File ID**.\n",
    "\n",
    "This is the most powerful way to share and use custom knowledge bases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- INSTRUCTION: UPLOAD test_brain.zip TO DRIVE FIRST ---\n",
    "# 1. Download the 'test_brain.zip' from the previous cell output.\n",
    "# 2. Upload it to your Google Drive.\n",
    "# 3. Replace the ID below with your new File ID.\n",
    "\n",
    "MY_CUSTOM_GDRIVE_ID = \"11rrfiYodPrQEIdshxAOQJnzf05QezSO_\" # <--- REPLACE THIS ID\n",
    "\n",
    "try:\n",
    "    # Initialize from Drive\n",
    "    drive_engine = GraphRAGEngine(gdrive_id=MY_CUSTOM_GDRIVE_ID)\n",
    "    print(\"\u2705 Custom Brain Loaded from Google Drive\")\n",
    "    \n",
    "    # Run the SAME query as above to verify consistency\n",
    "    solar_query = \"How does solar energy affect the grid stability?\"\n",
    "    drive_results = drive_engine.search(solar_query, top_k=2)\n",
    "\n",
    "    print(f\"\\n\ud83d\udd0d Query: {solar_query}\")\n",
    "    print(f\"Found {len(drive_results['papers'])} papers and {len(drive_results['graph_links'])} links.\")\n",
    "\n",
    "    if drive_results['graph_links']:\n",
    "        link = drive_results['graph_links'][0]\n",
    "        print(f\"\ud83d\udd17 Link Found: {link['node1']} --[{link['relation']}]--> {link['node2']}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\u274c Error loading custom brain: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}