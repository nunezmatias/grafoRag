{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåç GraphRAG Core: Climate Intelligence Tutorial\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nunezmatias/grafoRag/blob/main/examples/Tutorial_GraphRAG.ipynb)\n",
    "\n",
    "Welcome to the **GraphRAG Core** tutorial. This notebook demonstrates a next-generation retrieval system designed for scientific discovery. Unlike traditional search engines that return isolated documents, this system understands the *structure* of knowledge.\n",
    "\n",
    "By combining vector search with a causal knowledge graph, we can answer complex questions about climate adaptation, identifying not just *what* is happening, but *why* it matters and what ripple effects it might trigger.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation\n",
    "We will install the `graphrag_core` library directly from the repository. This package includes the retrieval engine and automatically handles the download of the Climate Knowledge Graph (~300MB).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/nunezmatias/grafoRag.git\n",
    "!pip install -q -U google-genai\n",
    "\n",
    "import os\n",
    "from graphrag_core import GraphRAGEngine\n",
    "print('‚úÖ Libraries Installed & Loaded')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize the Engine\n",
    "Initializing the engine is simple. If the climate data is not found locally, it will be automatically downloaded from the cloud storage. This ensures you have the latest version of the knowledge graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = GraphRAGEngine()\n",
    "# Watch the output below for the download progress bar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run a Deep Research Query\n",
    "We will now perform a complex search. The engine allows you to tune the depth of the investigation:\n",
    "\n",
    "- **`top_k`** establishes the **Breadth** of the investigation. It scans the entire library to find the core concepts related to your query.\n",
    "- **`context_k`** dictates the **Depth**. Instead of one paper, the engine reads multiple documents per topic to find consensus and nuance.\n",
    "- **`hops`** activates the **Causal Reasoning** layer. It follows the connections in the graph to find cascading risks.\n",
    "\n",
    "A configuration of `hops=2` allows us to see second-order effects, essential for systemic analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your research question\n",
    "query = \"cascading risks of extreme heat and urban floods\"\n",
    "\n",
    "# Execute the Search\n",
    "results = engine.search(\n",
    "    query=query, \n",
    "    top_k=3,        # Breadth\n",
    "    context_k=4,    # Depth\n",
    "    hops=2          # Causality\n",
    ")\n",
    "\n",
    "print(\"--- Research Stats ---\")\n",
    "print(f\"Primary Sources: {results['stats']['primary']}\")\n",
    "print(f\"Context Expansion: {results['stats']['context']}\")\n",
    "print(f\"Causal Links:      {results['stats']['graph']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inspect the Retrieved Intelligence\n",
    "It is important to verify the quality of the retrieved data. This \"White Box\" approach builds trust by showing you the exact evidence found before the AI summarizes it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Check the Top Paper\n",
    "if results['papers']:\n",
    "    p = results['papers'][0]\n",
    "    title = p['title']\n",
    "    content = p['content'][:200]\n",
    "    print(f'Top Paper: {title}')\n",
    "    print(f'Snippet: {content}...')\n",
    "\n",
    "# 2. Check Discovered Causal Chains\n",
    "if results['graph_links']:\n",
    "    print('\nSample Causal Chains Discovered:')\n",
    "    for link in results['graph_links'][:5]:\n",
    "        n1 = link['node1']\n",
    "        n2 = link['node2']\n",
    "        rel = link['relation']\n",
    "        print(f'   {n1} --[{rel}]--> {n2}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Construct the Expert Prompt\n",
    "We use the engine's built-in expert template to package this structured data into a rigorous prompt for the LLM. This template forces the model to triangulate evidence and cite specific sources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the default expert template designed for this Climate Graph\n",
    "prompt = engine.format_prompt(results, query)\n",
    "\n",
    "print(\"Here is your optimized prompt (COPY THIS):\")\n",
    "print(\"--------------------------------------------------\")\n",
    "print(prompt)\n",
    "print(\"--------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Answer with Gemini Flash ‚ö°\n",
    "Finally, we send the generated prompt to Google's Gemini model to synthesize the final report.\n",
    "\n",
    "**Prerequisite:** Add your API Key to Colab Secrets (Key icon on the left) with the name `GOOGLE_API_KEY`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.colab import userdata\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "try:\n",
    "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
    "    client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "    print('‚úÖ Gemini Client Configured')\n",
    "except Exception as e:\n",
    "    print('‚ö†Ô∏è Error: API Key not found in Colab Secrets.')\n",
    "\n",
    "print('‚è≥ Generating expert response with Gemini Flash...')\n",
    "try:\n",
    "    response = client.models.generate_content(\n",
    "        model='gemini-flash-latest',\n",
    "        contents=prompt\n",
    "    )\n",
    "    display(Markdown('### ü§ñ Response:'))\n",
    "    display(Markdown(response.text))\n",
    "except Exception as e:\n",
    "    print(f'‚ùå Error: {e}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced: Build Your Own Prompt Template\n",
    "Do you want full control? Here is how you can access the raw variables `papers` and `graph_links` to modify the prompt structure entirely before sending it to the LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_role = \"You are a Data Journalist writing for a general audience.\"\n",
    "my_instruction = \"Summarize the risks in 3 bullet points. Be concise.\"\n",
    "\n",
    "# 1. Flatten the Papers data into a string\n",
    "papers_text = \"\"\n",
    "for p in results['papers']:\n",
    "    t = p['title']\n",
    "    c = p['content'][:200]\n",
    "    papers_text += f'- {t}: {c}...\n'\n",
    "\n",
    "# 2. Flatten the Graph data into a string\n",
    "graph_text = \"\"\n",
    "for link in results['graph_links']:\n",
    "    n1 = link['node1']\n",
    "    n2 = link['node2']\n",
    "    graph_text += f'- {n1} causes {n2}\n'\n",
    "\n",
    "# 3. Build the F-String (Edit this!)\n",
    "custom_prompt = f\"ROLE: {my_role}\nQUESTION: {query}\n\nDATA:\n{papers_text}\n{graph_text}\n\nDO: {my_instruction}\"\n",
    "print(\"Custom prompt created successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Bonus: Swap the Brain üß† (Test a New Dataset)\n",
    "You can swap the underlying \"Brain\" instantly by providing a new Vector DB and Graph JSON.\n",
    "\n",
    "In this example, we will download a small **Test Brain** about Solar Energy from the repository and initialize a new engine with it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Download the Test Brain (Solar Energy) from GitHub\n",
    "!wget -q https://github.com/nunezmatias/grafoRag/raw/main/examples/test_brain.zip\n",
    "!unzip -o -q test_brain.zip\n",
    "\n",
    "# 2. Initialize a NEW Engine with this data\n",
    "solar_engine = GraphRAGEngine(\n",
    "    vector_db_path='./test_brain/test_db',\n",
    "    graph_json_path='./test_brain/test_skeleton.json'\n",
    ")\n",
    "\n",
    "# 3. Run a Query on the new domain\n",
    "solar_query = 'How does solar energy affect the grid stability?'\n",
    "solar_results = solar_engine.search(solar_query, top_k=2)\n",
    "\n",
    "print(f'Query: {solar_query}')\n",
    "p_count = len(solar_results['papers'])\n",
    "l_count = len(solar_results['graph_links'])\n",
    "print(f'Found {p_count} papers and {l_count} links.')\n",
    "\n",
    "if solar_results['graph_links']:\n",
    "    link = solar_results['graph_links'][0]\n",
    "    n1, n2, rel = link['node1'], link['node2'], link['relation']\n",
    "    print(f'Link Found: {n1} --[{rel}]--> {n2}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Advanced: Load your own Brain from Google Drive ‚òÅÔ∏è\n",
    "If you have your Knowledge Graph and Vector DB packaged in a `.zip` and hosted on Google Drive, you can load it directly by passing its **File ID**.\n",
    "\n",
    "**Note:** Ensure you upload your own `test_brain.zip` to Drive and replace the ID below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the ID below with your own File ID from Google Drive\n",
    "MY_CUSTOM_GDRIVE_ID = \"1iKcEzECN9LTMi3bIq4ocRfFJgvb1dLus\"\n",
    "\n",
    "try:\n",
    "    # Initialize from Drive\n",
    "    drive_engine = GraphRAGEngine(gdrive_id=MY_CUSTOM_GDRIVE_ID)\n",
    "    print('‚úÖ Custom Brain Loaded from Google Drive')\n",
    "    \n",
    "    # Run the SAME query as above to verify consistency\n",
    "    solar_query = 'How does solar energy affect the grid stability?'\n",
    "    drive_results = drive_engine.search(solar_query, top_k=2)\n",
    "\n",
    "    print(f'Query: {solar_query}')\n",
    "    p_count = len(drive_results['papers'])\n",
    "    l_count = len(drive_results['graph_links'])\n",
    "    print(f'Found {p_count} papers and {l_count} links.')\n",
    "\n",
    "    if drive_results['graph_links']:\n",
    "        link = drive_results['graph_links'][0]\n",
    "        n1, n2, rel = link['node1'], link['node2'], link['relation']\n",
    "        print(f'Link Found: {n1} --[{rel}]--> {n2}')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'‚ùå Error: {e}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}