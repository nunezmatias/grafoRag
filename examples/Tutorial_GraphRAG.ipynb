{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83c\udf0d GraphRAG Core: Climate Intelligence Tutorial\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nunezmatias/grafoRag/blob/main/examples/Tutorial_GraphRAG.ipynb)\n",
    "\n",
    "Welcome to the **GraphRAG Core** tutorial. This library comes **pre-loaded** with a massive Climate Adaptation Knowledge Graph, allowing you to perform deep scientific research instantly.\n",
    "\n",
    "### \ud83d\ude80 Features\n",
    "1. **Plug & Play**: No data setup required. The Climate Knowledge Base is embedded (auto-downloaded).\n",
    "2. **Deep Retrieval**: Finds not just one paper, but the entire context around a topic.\n",
    "3. **Causal Reasoning**: Traverses the graph to find systemic risks (e.g., Heat -> Drought -> Food Insecurity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation\n",
    "Install the library directly from GitHub. This will setup the engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/nunezmatias/grafoRag.git\n",
    "!pip install -q -U google-genai\n",
    "\n",
    "import os\n",
    "from graphrag_core import GraphRAGEngine\n",
    "print(\"\u2705 Libraries Installed & Loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize the Engine\n",
    "We initialize the engine without arguments. It will automatically detect missing data and download the Climate Database (~300MB) from the cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = GraphRAGEngine()\n",
    "# Output should say: \"Attempting to download from Google Drive...\" followed by \"System Ready\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run a Deep Research Query\n",
    "We will now perform a complex search. The engine allows you to tune the depth of the investigation:\n",
    "\n",
    "- **`top_k`** controls breadth (how many distinct topics to start with).\n",
    "- **`context_k`** controls depth (how many papers to read per topic).\n",
    "- **`hops`** controls causal reasoning (how far to travel in the graph).\n",
    "\n",
    "A configuration of `hops=2` allows us to see second-order effects, essential for understanding cascading risks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your research question\n",
    "query = \"cascading risks of extreme heat and urban floods\"\n",
    "\n",
    "# Execute the Search\n",
    "results = engine.search(\n",
    "    query=query, \n",
    "    top_k=3,        # Breadth\n",
    "    context_k=4,    # Depth\n",
    "    hops=2          # Causality\n",
    ")\n",
    "\n",
    "print(f\"--- Research Stats ---\")\n",
    "print(f\"Primary Sources: {results['stats']['primary']}\")\n",
    "print(f\"Context Expansion: {results['stats']['context']}\")\n",
    "print(f\"Causal Links:      {results['stats']['graph']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inspect the Intelligence\n",
    "It is important to verify the quality of the retrieved data. Below, we print the top-ranked paper and a sample of the causal chains discovered by the graph traversal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Check the Top Paper\n",
    "if results['papers']:\n",
    "    p = results['papers'][0]\n",
    "    print(f\"\ud83d\udcc4 Top Paper: {p['title']}\")\n",
    "    print(f\"   Snippet: {p['content'][:200]}...\")\n",
    "\n",
    "# 2. Check Discovered Causal Chains\n",
    "if results['graph_links']:\n",
    "    print(\"\n\ud83d\udd17 Sample Causal Chains:\")\n",
    "    for link in results['graph_links'][:5]:\n",
    "        print(f\"   {link['node1']} --[{link['relation']}]--> {link['node2']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Construct the Expert Prompt\n",
    "We use the engine's built-in expert template to package this structured data into a rigorous prompt for the LLM. This template forces the model to triangulate evidence and cite sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the default expert template designed for this Climate Graph\n",
    "prompt = engine.format_prompt(results, query)\n",
    "\n",
    "print(\"Here is your optimized prompt (COPY THIS):\")\n",
    "print(\"--------------------------------------------------\")\n",
    "print(prompt)\n",
    "print(\"--------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Answer with Gemini Flash \u26a1\n",
    "Finally, we send the generated prompt to Google's Gemini model.\n",
    "\n",
    "**Prerequisite:** Add your API Key to Colab Secrets (Key icon on the left) with the name `GOOGLE_API_KEY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.colab import userdata\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Configuraci\u00f3n de la API Key\n",
    "try:\n",
    "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
    "    client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "    print(\"\u2705 Gemini Client Configured\")\n",
    "except Exception as e:\n",
    "    print(\"\u26a0\ufe0f Error: API Key not found. Please add 'GOOGLE_API_KEY' to Colab Secrets.\")\n",
    "\n",
    "# Generar contenido\n",
    "print(\"\u23f3 Generating expert response with Gemini 2.0 Flash...\")\n",
    "try:\n",
    "    response = client.models.generate_content(\n",
    "        model='gemini-2.0-flash',\n",
    "        contents=prompt\n",
    "    )\n",
    "    \n",
    "    # Mostrar la respuesta formateada\n",
    "    display(Markdown(\"### \ud83e\udd16 Response:\"))\n",
    "    display(Markdown(response.text))\n",
    "except Exception as e:\n",
    "    print(f\"\u274c Generation Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced: Build Your Own Prompt Template\n",
    "Do you want full control? Here is how you can access the raw variables `papers` and `graph_links` to modify the prompt structure entirely before sending it to the LLM.\n",
    "\n",
    "You can edit the F-String below to change the persona, instructions, or layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CUSTOMIZE YOUR TEMPLATE HERE ---my_role = \"You are a Data Journalist writing for a general audience.\"my_instruction = \"Summarize the risks in 3 bullet points. Be concise.\"\n",
    "# 1. Flatten the Papers data into a string\n",
    "papers_text = \"\"for p in results['papers']:\n",
    "    papers_text += f\"- {p['title']}: {p['content'][:200]}...\n\"\n",
    "# 2. Flatten the Graph data into a string\n",
    "graph_text = \"\"for link in results['graph_links']:\n",
    "    graph_text += f\"- {link['node1']} causes {link['node2']}\n\"\n",
    "# 3. Build the F-String (Edit this!)\n",
    "custom_prompt = (f\"ROLE: {my_role}\n\"                 f\"QUESTION: {query}\n\n\"                 f\"SCIENTIFIC EVIDENCE:\n{papers_text}\n\"                 f\"CAUSAL LINKS:\n{graph_text}\n\"                 f\"INSTRUCTION: {my_instruction}\")\n",
    "\n",
    "print(\"--- Custom Prompt Created ---\")\n",
    "print(custom_prompt[:500] + \"... [Truncated]\")\n",
    "\n",
    "# 4. Send Custom Prompt to Gemini\n",
    "print(\"\n\u23f3 Generating Custom Response...\")\n",
    "try:\n",
    "    response = client.models.generate_content(\n",
    "        model='gemini-2.0-flash',\n",
    "        contents=custom_prompt\n",
    "    )\n",
    "    display(Markdown(\"### \ud83e\udd16 Custom Response:\"))\n",
    "    display(Markdown(response.text))\n",
    "except Exception as e:\n",
    "    print(f\"\u274c Generation Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}