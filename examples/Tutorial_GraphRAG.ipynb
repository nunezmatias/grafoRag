{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83c\udf0d GraphRAG Core: Climate Intelligence Tutorial\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nunezmatias/grafoRag/blob/main/examples/Tutorial_GraphRAG.ipynb)\n",
    "\n",
    "Welcome to the **GraphRAG Core** tutorial. This notebook demonstrates a next-generation retrieval system designed for scientific discovery. Unlike traditional search engines that return isolated documents, this system understands the *structure* of knowledge.\n",
    "\n",
    "By combining vector search with a causal knowledge graph, we can answer complex questions about climate adaptation, identifying not just *what* is happening, but *why* it matters and what ripple effects it might trigger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation\n",
    "We will install the `graphrag_core` library directly from the repository. This package includes the retrieval engine and automatically handles the download of the Climate Knowledge Graph (~300MB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/nunezmatias/grafoRag.git\n",
    "!pip install -q -U google-genai\n",
    "\n",
    "import os\n",
    "from graphrag_core import GraphRAGEngine\n",
    "print(\"\u2705 Libraries Installed & Loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize the Engine\n",
    "Initializing the engine is simple. If the climate data is not found locally, it will be automatically downloaded from the cloud storage. This ensures you have the latest version of the knowledge graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = GraphRAGEngine()\n",
    "# Watch the output below for the download progress bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run a Deep Research Query\n",
    "We will now perform a complex search. The engine allows you to tune the depth of the investigation:\n",
    "\n",
    "- **`top_k`** controls breadth (how many distinct topics to start with).\n",
    "- **`context_k`** controls depth (how many papers to read per topic).\n",
    "- **`hops`** controls causal reasoning (how far to travel in the graph).\n",
    "\n",
    "A configuration of `hops=2` allows us to see second-order effects, essential for understanding cascading risks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your research question\n",
    "query = \"cascading risks of extreme heat and urban floods\"\n",
    "\n",
    "# Execute the Search\n",
    "results = engine.search(\n",
    "    query=query, \n",
    "    top_k=3,        # Breadth\n",
    "    context_k=4,    # Depth\n",
    "    hops=2          # Causality\n",
    ")\n",
    "\n",
    "print(f\"--- Research Stats ---\")\n",
    "print(f\"Primary Sources: {results['stats']['primary']}\")\n",
    "print(f\"Context Expansion: {results['stats']['context']}\")\n",
    "print(f\"Causal Links:      {results['stats']['graph']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inspect the Intelligence\n",
    "It is important to verify the quality of the retrieved data. Below, we print the top-ranked paper and a sample of the causal chains discovered by the graph traversal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Check the Top Paper\n",
    "if results['papers']:\n",
    "    p = results['papers'][0]\n",
    "    print(f\"\ud83d\udcc4 Top Paper: {p['title']}\")\n",
    "    print(f\"   Snippet: {p['content'][:200]}...\")\n",
    "\n",
    "# 2. Check Discovered Causal Chains\n",
    "if results['graph_links']:\n",
    "    print(\"\n\ud83d\udd17 Sample Causal Chains:\")\n",
    "    for link in results['graph_links'][:5]:\n",
    "        print(f\"   {link['node1']} --[{link['relation']}]--> {link['node2']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Construct the Expert Prompt\n",
    "We use the engine's built-in expert template to package this structured data into a rigorous prompt for the LLM. This template forces the model to triangulate evidence and cite sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the default expert template designed for this Climate Graph\n",
    "prompt = engine.format_prompt(results, query)\n",
    "\n",
    "print(\"Here is your optimized prompt (COPY THIS):\")\n",
    "print(\"--------------------------------------------------\")\n",
    "print(prompt)\n",
    "print(\"--------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Answer with Gemini Flash \u26a1\n",
    "Finally, we send the generated prompt to Google's Gemini model. \n",
    "\n",
    "> **Tip:** If you want to modify the prompt (e.g., change the role or add specific constraints), simply edit the string in the `prompt` variable before running this cell.\n",
    "\n",
    "**Prerequisite:** Add your API Key to Colab Secrets (Key icon on the left) with the name `GOOGLE_API_KEY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.colab import userdata\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Configuraci\u00f3n de la API Key\n",
    "try:\n",
    "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
    "    client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "    print(\"\u2705 Gemini Client Configured\")\n",
    "except Exception as e:\n",
    "    print(\"\u26a0\ufe0f Error: API Key not found. Please add 'GOOGLE_API_KEY' to Colab Secrets.\")\n",
    "\n",
    "# Generar contenido\n",
    "print(\"\u23f3 Generating expert response with Gemini 2.0 Flash...\")\n",
    "try:\n",
    "    response = client.models.generate_content(\n",
    "        model='gemini-2.0-flash',\n",
    "        contents=prompt\n",
    "    )\n",
    "    \n",
    "    # Mostrar la respuesta formateada\n",
    "    display(Markdown(\"### \ud83e\udd16 Response:\"))\n",
    "    display(Markdown(response.text))\n",
    "except Exception as e:\n",
    "    print(f\"\u274c Generation Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}