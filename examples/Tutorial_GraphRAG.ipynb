{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83c\udf0d GraphRAG Core: Climate Intelligence Tutorial\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nunezmatias/grafoRag/blob/main/examples/Tutorial_GraphRAG.ipynb)\n",
    "\n",
    "Welcome to the **GraphRAG Core** tutorial. This library comes **pre-loaded** with a massive Climate Adaptation Knowledge Graph, allowing you to perform deep scientific research instantly.\n",
    "\n",
    "### \ud83d\ude80 Features\n",
    "1. **Plug & Play**: No data setup required. The Climate Knowledge Base is embedded (auto-downloaded).\n",
    "2. **Deep Retrieval**: Finds not just one paper, but the entire context around a topic.\n",
    "3. **Causal Reasoning**: Traverses the graph to find systemic risks (e.g., Heat -> Drought -> Food Insecurity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation\n",
    "Install the library directly from GitHub. This will setup the engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/nunezmatias/grafoRag.git\n",
    "!pip install -q -U google-genai\n",
    "\n",
    "import os\n",
    "from graphrag_core import GraphRAGEngine\n",
    "print(\"\u2705 Libraries Installed & Loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize the Engine\n",
    "We initialize the engine without arguments. It will automatically detect missing data and download the Climate Database (~300MB) from the cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = GraphRAGEngine()\n",
    "# Output should say: \"Attempting to download from Google Drive...\" followed by \"System Ready\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run a Deep Research Query\n",
    "We will now perform a complex search. The engine allows you to tune the depth of the investigation:\n",
    "\n",
    "- **`top_k`** controls breadth (how many distinct topics to start with).\n",
    "- **`context_k`** controls depth (how many papers to read per topic).\n",
    "- **`hops`** controls causal reasoning (how far to travel in the graph).\n",
    "\n",
    "A configuration of `hops=2` allows us to see second-order effects, essential for understanding cascading risks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your research question",
    "query = \"cascading risks of extreme heat and urban floods\"",
    "",
    "# Execute the Search",
    "results = engine.search(",
    "    query=query, ",
    "    top_k=3,        # Breadth",
    "    context_k=4,    # Depth",
    "    hops=2          # Causality",
    ")",
    "",
    "print(f\"--- Research Stats ---\")",
    "print(f\"Primary Sources: {results['stats']['primary']}\")",
    "print(f\"Context Expansion: {results['stats']['context']}\")",
    "print(f\"Causal Links:      {results['stats']['graph']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inspect the Intelligence\n",
    "It is important to verify the quality of the retrieved data. Below, we print the top-ranked paper and a sample of the causal chains discovered by the graph traversal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Check the Top Paper",
    "if results['papers']:",
    "    p = results['papers'][0]",
    "    print(f\"\ud83d\udcc4 Top Paper: {p['title']}\")",
    "    print(f\"   Snippet: {p['content'][:200]}...\")",
    "",
    "# 2. Check Discovered Causal Chains",
    "if results['graph_links']:",
    "    print(\"\\n\ud83d\udd17 Sample Causal Chains:\")",
    "    for link in results['graph_links'][:5]:",
    "        print(f\"   {link['node1']} --[{link['relation']}]--> {link['node2']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Construct the Expert Prompt\n",
    "We use the engine's built-in expert template to package this structured data into a rigorous prompt for the LLM. This template forces the model to triangulate evidence and cite sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the default expert template designed for this Climate Graph\n",
    "prompt = engine.format_prompt(results, query)\n",
    "\n",
    "print(\"Here is your optimized prompt (COPY THIS):\")\n",
    "print(\"--------------------------------------------------\")\n",
    "print(prompt)\n",
    "print(\"--------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Answer with Gemini Flash \u26a1\n",
    "Finally, we send the generated prompt to Google's Gemini model.\n",
    "\n",
    "**Prerequisite:** Add your API Key to Colab Secrets (Key icon on the left) with the name `GOOGLE_API_KEY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.colab import userdata\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Configuraci\u00f3n de la API Key\n",
    "try:\n",
    "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
    "    client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "    print(\"\u2705 Gemini Client Configured\")\n",
    "except Exception as e:\n",
    "    print(\"\u26a0\ufe0f Error: API Key not found. Please add 'GOOGLE_API_KEY' to Colab Secrets.\")\n",
    "\n",
    "# Generar contenido\n",
    "print(\"\u23f3 Generating expert response with Gemini 2.0 Flash...\")\n",
    "try:\n",
    "    response = client.models.generate_content(\n",
    "        model='gemini-2.0-flash',\n",
    "        contents=prompt\n",
    "    )\n",
    "    \n",
    "    # Mostrar la respuesta formateada\n",
    "    display(Markdown(\"### \ud83e\udd16 Response:\"))\n",
    "    display(Markdown(response.text))\n",
    "except Exception as e:\n",
    "    print(f\"\u274c Generation Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced: Build Your Own Prompt Template\n",
    "Do you want full control? Here is how you can access the raw variables `papers` and `graph_links` to modify the prompt structure entirely before sending it to the LLM.\n",
    "\n",
    "You can edit the F-String below to change the persona, instructions, or layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CUSTOMIZE YOUR TEMPLATE HERE ---",
    "my_role = \"You are a Data Journalist writing for a general audience.\"",
    "my_instruction = \"Summarize the risks in 3 bullet points. Be concise.\"",
    "",
    "# 1. Flatten the Papers data into a string",
    "papers_text = \"\"",
    "for p in results['papers']:",
    "    papers_text += f\"- {p['title']}: {p['content'][:200]}...\\n\"",
    "",
    "# 2. Flatten the Graph data into a string",
    "graph_text = \"\"",
    "for link in results['graph_links']:",
    "    graph_text += f\"- {link['node1']} causes {link['node2']}\\n\"",
    "",
    "# 3. Build the F-String (Edit this!)",
    "custom_prompt = (f\"ROLE: {my_role}\\n\"",
    "                 f\"QUESTION: {query}\\n\\n\"",
    "                 f\"SCIENTIFIC EVIDENCE:\\n{papers_text}\\n\"",
    "                 f\"CAUSAL LINKS:\\n{graph_text}\\n\"",
    "                 f\"INSTRUCTION: {my_instruction}\")",
    "",
    "print(\"--- Custom Prompt Created ---\")",
    "print(custom_prompt[:500] + \"... [Truncated]\")",
    "",
    "# 4. Send Custom Prompt to Gemini",
    "print(\"\\n\u23f3 Generating Custom Response...\")",
    "try:",
    "    response = client.models.generate_content(",
    "        model='gemini-2.0-flash',",
    "        contents=custom_prompt",
    "    )",
    "    display(Markdown(\"### \ud83e\udd16 Custom Response:\"))",
    "    display(Markdown(response.text))",
    "except Exception as e:",
    "    print(f\"\u274c Generation Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Bonus: Swap the Brain \ud83e\udde0 (Test a New Dataset)\n",
    "Do you want to use this engine for a different domain (e.g., Solar Energy, Finance, Law)?\n",
    "You can swap the underlying \"Brain\" instantly by providing a new Vector DB and Graph JSON.\n",
    "\n",
    "In this example, we will download a small **Test Brain** about Solar Energy from the repository and initialize a new engine with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Download the Test Brain (Solar Energy) from GitHub",
    "!wget https://github.com/nunezmatias/grafoRag/raw/main/examples/test_brain.zip",
    "!unzip -o test_brain.zip",
    "",
    "print(\"\u2705 Test Brain Downloaded & Extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Initialize a NEW Engine with this data",
    "solar_engine = GraphRAGEngine(",
    "    vector_db_path=\"./test_brain/test_db\",",
    "    graph_json_path=\"./test_brain/test_skeleton.json\"",
    ")",
    "",
    "# 3. Run a Query on the new domain",
    "solar_query = \"How does solar energy affect the grid stability?\"",
    "",
    "solar_results = solar_engine.search(solar_query, top_k=2)",
    "",
    "print(f\"\\n\ud83d\udd0d Query: {solar_query}\")",
    "print(f\"Found {len(solar_results['papers'])} papers and {len(solar_results['graph_links'])} links.\")",
    "",
    "if solar_results['graph_links']:",
    "    link = solar_results['graph_links'][0]",
    "    print(f\"\ud83d\udd17 Link Found: {link['node1']} --[{link['relation']}]--> {link['node2']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Advanced: Load your own Brain from Google Drive \u2601\ufe0f\n",
    "If you have your Knowledge Graph and Vector DB packaged in a `.zip` and hosted on Google Drive, you can load it directly by passing its **File ID**.\n",
    "\n",
    "This is the most powerful way to share and use custom knowledge bases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example using a custom Google Drive ID (Same Solar Brain for comparison)",
    "MY_CUSTOM_GDRIVE_ID = \"11rrfiYodPrQEIdshxAOQJnzf05QezSO_\"",
    "",
    "try:",
    "    # Initialize from Drive",
    "    drive_engine = GraphRAGEngine(gdrive_id=MY_CUSTOM_GDRIVE_ID)",
    "    print(\"\u2705 Custom Brain Loaded from Google Drive\")",
    "    ",
    "    # Run the SAME query as above to verify consistency",
    "    solar_query = \"How does solar energy affect the grid stability?\"",
    "    drive_results = drive_engine.search(solar_query, top_k=2)",
    "",
    "    print(f\"\\n\ud83d\udd0d Query: {solar_query}\")",
    "    print(f\"Found {len(drive_results['papers'])} papers and {len(drive_results['graph_links'])} links.\")",
    "",
    "    if drive_results['graph_links']:",
    "        link = drive_results['graph_links'][0]",
    "        print(f\"\ud83d\udd17 Link Found: {link['node1']} --[{link['relation']}]--> {link['node2']}\")",
    "",
    "except Exception as e:",
    "    print(f\"\u274c Error loading custom brain: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}